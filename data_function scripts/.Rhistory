print("hello")
#everything related to statistical calculations of networks
#and plots
#remember to set directory to 'data_function scripts' folder
#load libraries
library(tidyverse)
library(quanteda) #for text cleaning
library(igraph) #for creating graphs
library(visNetwork) #for visualizing graphs
library(wordcloud) #for creating wordclouds
#load_functions
source("calculatecoocstats.R") #calculate co-occurrence statistics
source("grapher.R") #create graph
#Wiedemann, Gregor; Niekler, Andreas (2017): Hands-on: A five day text mining course for humanists and social scientists in R. Proceedings of the 1st Workshop on Teaching NLP for Digital Humanities (Teach4DH@GSCL 2017), Berlin.
source("rawcounts.R") #find raw counts of co-occurrences
source("token_filter.R") #filter tokens
#load tokens, get it ready for analysis
load("token.all.RData")
#convert tokens to all lower
token.all <- tokens_tolower(token.all) #convert all tokens to lower
#create tokens for males
toks.male <- token.all %>%
tokens_select(pattern = 'male/characters', padding = FALSE, window = 5)
setwd("~/Documents/GitHub/film_networks/data_function scripts")
#everything related to statistical calculations of networks
#and plots
#remember to set directory to 'data_function scripts' folder
#load libraries
library(tidyverse)
library(quanteda) #for text cleaning
library(igraph) #for creating graphs
library(visNetwork) #for visualizing graphs
library(wordcloud) #for creating wordclouds
#load_functions
source("calculatecoocstats.R") #calculate co-occurrence statistics
source("grapher.R") #create graph
#Wiedemann, Gregor; Niekler, Andreas (2017): Hands-on: A five day text mining course for humanists and social scientists in R. Proceedings of the 1st Workshop on Teaching NLP for Digital Humanities (Teach4DH@GSCL 2017), Berlin.
source("rawcounts.R") #find raw counts of co-occurrences
source("token_filter.R") #filter tokens
#load tokens, get it ready for analysis
load("token.all.RData")
#convert tokens to all lower
token.all <- tokens_tolower(token.all) #convert all tokens to lower
#create tokens for males
toks.male <- token.all %>%
tokens_select(pattern = 'male/characters', padding = FALSE, window = 5)
male_fcmat = fcm(toks.male, context = c("window"),
count = c("frequency"),
window = 5)
male_fcmat
graph_m = graph_from_adjacency_matrix(male_fcmat, mode = "undirected")
graph_m = simplify(graph_m)
graph_m.comm <- cluster_fast_greedy(graph_m)
graph_m.comm
minimumFrequency = 25
binDTM <- toks.male %>%
dfm() %>%
dfm_trim(min_docfreq = minimumFrequency) %>%
dfm_weight("count")
comat <- t(binDTM) %*% binDTM
film_graph <- graph_from_adjacency_matrix(comat, mode = "undirected")
#plot.igraph(film_graph, vertex.label = NA, vertex.size = 8)
film_graph <- simplify(film_graph, remove.loops = TRUE)
c1 = cluster_fast_greedy(film_graph)
modularity(c1)
length(c1)
sizes(c1)
c1
head((c1[[4]]))
head((c1[[1]]))
head((c1[[2]]))
head((c1[[3]]))
# modularity matrix
plot.igraph(film_graph, vertex.color=membership(c1), vertex.label = NA, vertex.size = 10)
# Contract vertices
E(film_graph)$weight <- 1
V(film_graph)$weight <- 1
gcon <- contract.vertices(film_graph, c1$membership,
vertex.attr.comb = list(weight = "sum", name = function(x)x[1], "ignore"))
# Simplify edges
gcon <- simplify(gcon, edge.attr.comb = list(weight = "sum", function(x)length(x)))
gcc <- induced.subgraph(gcon, V(gcon)$weight > 20)
V(gcc)$degree <- unname(degree(gcc))
set.seed(42)
par(mar = rep(0.1, 4))
g.layout <- layout.kamada.kawai(gcc)
plot.igraph(gcc, edge.arrow.size = 0.1, layout = g.layout, vertex.size = 0.5 * (V(gcc)$degree))
head((c1[[3]]))
minimumFrequency = 10
binDTM <- toks.male %>%
dfm() %>%
dfm_trim(min_docfreq = minimumFrequency) %>%
dfm_weight("count")
comat <- t(binDTM) %*% binDTM
film_graph <- graph_from_adjacency_matrix(comat, mode = "undirected")
#plot.igraph(film_graph, vertex.label = NA, vertex.size = 8)
film_graph <- simplify(film_graph, remove.loops = TRUE)
c1 = cluster_fast_greedy(film_graph)
modularity(c1)
modularity(c1)
length(c1)
sizes(c1)
c1
graphf
#community structure
graphf = graph.f[[1]] #store graph object
graphm
#community structure
#male
graphm = graph.m[[1]] #store graph object
library(tidyverse)
library(quanteda) #for text cleaning
library(igraph) #for creating graphs
library(visNetwork) #for visualizing graphs
library(wordcloud) #for creating wordclouds
#load_functions
source("calculatecoocstats.R") #calculate co-occurrence statistics
source("grapher.R") #create graph
#Wiedemann, Gregor; Niekler, Andreas (2017): Hands-on: A five day text mining course for humanists and social scientists in R. Proceedings of the 1st Workshop on Teaching NLP for Digital Humanities (Teach4DH@GSCL 2017), Berlin.
source("rawcounts.R") #find raw counts of co-occurrences
source("token_filter.R") #filter tokens
#load tokens, get it ready for analysis
load("token.all.RData")
#convert tokens to all lower
token.all <- tokens_tolower(token.all) #convert all tokens to lower
#create a token set with only generalized pos info
pos_replace <- function(toks.replace){
toks.replace <- toks.replace %>%
tokens_replace(pattern = c("*/NOUN", "*/VERB", "*/ADJ"), replacement = c("NOUN", "VERB", "ADJ"))
return(toks.replace)
}
token.pos <- pos_replace(token.all)
#community structure
#male
graphm = graph.m[[1]] #store graph object
vism <- toVisNetworkData(graphm) #create visnetwork object
nodes <- vism$nodes #store nodes
#male
male.perm <- data.frame() #initialize
graph.m = grapher("male/characters", 10, token_filter2("all", 1940, 2010, token.all)) #extract graph info
gr.m <- graph.m[[3]] #pass graph object
gr.m <- gr.m[gr.m$names != "female/characters",] #filter out female characters
gr.m <- gr.m[1:20,] #filter 20
gr.m$rank = 1 : nrow(gr.m) #rank
gr.m$gender = "male" #assign gender
source("~/Documents/GitHub/film_networks/main_script.R", echo=TRUE)
#community structure
#male
graphm = graph.m[[1]] #store graph object
#community structure
#male
graphm = graph.m[[1]] #store graph object
vism <- toVisNetworkData(graphm) #create visnetwork object
nodes <- vism$nodes #store nodes
nodes <- nodes %>% select(-color) #remove color nodes
edges <- vism$edges #store edges
graphm = simplify(graphm, remove.loops = TRUE) #remove loops
vism_comm <- cluster_fast_greedy(graphm) #find clusters using igraph
modularity(vism_comm) #modularity
plot(vism_comm, graphm, vertex.size = 4, vertex.label = NA)
graphm
degree(graph_m)
degree(graphm)
