print("hello")
setwd("~/Documents/GitHub/film_networks/data_function scripts")
#load libraries
library(tidyverse)
library(quanteda) #for text cleaning
library(igraph) #for creating graphs
library(visNetwork) #for visualizing graphs
library(wordcloud) #for creating wordclouds
#load_functions
source("calculatecoocstats.R") #calculate co-occurrence statistics
source("grapher.R") #create graph
source("graphervf.R")
source("rawcounts.R") #find raw counts of co-occurrences
source("token_filter.R") #filter tokens
#load tokens, get it ready for analysis
load("token.all.RData")
#convert tokens to all lower
token.all <- tokens_tolower(token.all) #convert all tokens to lower
#sample based on min in a decade
token.all = tokens_sample(token.all, size = 22638, replace = FALSE, prob = NULL, by = decade)
token.all = token.all %>% tokens_remove('ex/noun')
source("graphervf.R")
graph = graphervf(15, token_filter3("noun", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
graph = graphervf(15, token_filter3("noun", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
graph = graphervf(15, token_filter3("verb", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
graph = graphervf(15, token_filter3("adj", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
token.all = token.all %>% tokens_remove(c('ex/adj', 'ex/noun'))
graph = graphervf(15, token_filter3("adj", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
graph = graphervf(15, token_filter3("all", 1940, 2020, token.all))
graph_plot = visIgraph(graph[[1]]) %>% visNodes(font = list(size = 28))
graph_plot
